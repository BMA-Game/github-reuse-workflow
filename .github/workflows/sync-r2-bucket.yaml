name: Sync to R2 Bucket

on:
  workflow_call:
    inputs:
      directory:
        description: "The local directory to sync from"
        required: true
        default: "dev"
        type: string
      bucket-name:
        description: "The name of the R2 bucket"
        required: true
        type: string
      r2-account-id:
        description: "Your Cloudflare R2 Account ID"
        required: true
        type: string
    secrets:
      R2_ACCESS_KEY_ID:
        description: "Your R2 Access Key ID"
        required: true
      R2_SECRET_ACCESS_KEY:
        description: "Your R2 Secret Access Key"
        required: true

jobs:
  sync-to-r2:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS CLI for large file count
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: "auto"
        run: |
          # Optimized configuration for handling
          aws configure set default.s3.max_concurrent_requests 50
          aws configure set default.s3.max_bandwidth 1GB/s
          aws configure set default.s3.multipart_threshold 128MB
          aws configure set default.s3.multipart_chunksize 128MB
          aws configure set default.s3.max_queue_size 10000
          aws configure set default.s3.use_accelerate_endpoint false
          aws configure set default.s3.addressing_style virtual

      - name: Sync files to R2 Bucket
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: "auto"
        run: |
          echo "Starting sync of approximately"
          echo "Using optimized settings for large file count"

          # Pre-sync: List files to get accurate count and progress
          echo "Counting local files..."
          LOCAL_FILES=$(find ${{ inputs.directory }} -type f | wc -l)
          echo "Found $LOCAL_FILES local files to process"

          # Main sync - force update all files to ensure no corruption
          aws s3 sync ${{ inputs.directory }}/ s3://${{ inputs.bucket-name }} \
            --endpoint-url https://${{ inputs.r2-account-id }}.r2.cloudflarestorage.com \
            --cli-read-timeout 0 \
            --cli-connect-timeout 120 \
            --exclude "*.tmp" \
            --exclude "*.temp" \
            --exclude ".DS_Store" \
            --exclude "Thumbs.db" \
            --storage-class STANDARD \
            --delete \
            --no-progress 2>&1 | while read line; do
              echo "[$(date '+%Y-%m-%d %H:%M:%S')] $line"
            done

          echo "Main sync completed!"

      - name: Fix image content-types and verify integrity
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: "auto"
        run: |
          echo "Fixing content-types for image files..."

          # Get list of image files from bucket
          aws s3 ls s3://${{ inputs.bucket-name }} \
            --endpoint-url https://${{ inputs.r2-account-id }}.r2.cloudflarestorage.com \
            --recursive | grep -iE '\.(jpg|jpeg|png|gif|webp|svg|bmp|tiff|ico)$' > image_files.txt

          echo "Found $(wc -l < image_files.txt) image files"

          # Process each image file
          while IFS= read -r line; do
            if [ -n "$line" ]; then
              # Extract file path (4th column) and size (3rd column)
              file_path=$(echo "$line" | awk '{print $4}')
              file_size=$(echo "$line" | awk '{print $3}')
              
              if [ "$file_size" = "0" ]; then
                echo "WARNING: Zero-byte file detected: $file_path"
                # Try to re-upload from local
                local_file="${{ inputs.directory }}/$file_path"
                if [ -f "$local_file" ] && [ -s "$local_file" ]; then
                  echo "Re-uploading: $file_path"
                  aws s3 cp "$local_file" "s3://${{ inputs.bucket-name }}/$file_path" \
                    --endpoint-url https://${{ inputs.r2-account-id }}.r2.cloudflarestorage.com
                fi
              fi
              
              # Set proper content-type based on extension
              extension=$(echo "$file_path" | awk -F. '{print tolower($NF)}')
              case $extension in
                jpg|jpeg) content_type="image/jpeg" ;;
                png) content_type="image/png" ;;
                gif) content_type="image/gif" ;;
                webp) content_type="image/webp" ;;
                svg) content_type="image/svg+xml" ;;
                bmp) content_type="image/bmp" ;;
                tiff|tif) content_type="image/tiff" ;;
                ico) content_type="image/x-icon" ;;
                *) content_type="application/octet-stream" ;;
              esac
              
              # Update metadata for the file
              echo "Setting content-type $content_type for $file_path"
              aws s3 cp "s3://${{ inputs.bucket-name }}/$file_path" "s3://${{ inputs.bucket-name }}/$file_path" \
                --endpoint-url https://${{ inputs.r2-account-id }}.r2.cloudflarestorage.com \
                --content-type "$content_type" \
                --cache-control "public, max-age=31536000" \
                --metadata-directive REPLACE \
                --quiet || echo "Failed to update metadata for $file_path"
            fi
          done < image_files.txt

          echo "Image content-type fix completed!"

      - name: Final verification
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: "auto"
        run: |
          echo "Final verification..."

          # Count files
          REMOTE_FILES=$(aws s3 ls s3://${{ inputs.bucket-name }} --endpoint-url https://${{ inputs.r2-account-id }}.r2.cloudflarestorage.com --recursive | wc -l)
          echo "Remote bucket contains $REMOTE_FILES files"

          # Check for zero-byte files
          ZERO_BYTE_FILES=$(aws s3 ls s3://${{ inputs.bucket-name }} --endpoint-url https://${{ inputs.r2-account-id }}.r2.cloudflarestorage.com --recursive | awk '$3 == 0 {count++} END {print count+0}')
          echo "Zero-byte files: $ZERO_BYTE_FILES"

          # Summary
          echo "Sync Summary:"
          echo "- Total files in bucket: $REMOTE_FILES"
          echo "- Zero-byte files: $ZERO_BYTE_FILES"
          echo "- Sync completed at: $(date)"

          if [ "$ZERO_BYTE_FILES" -gt 0 ]; then
            echo "WARNING: There are still zero-byte files. Check the logs above."
            exit 1
          else
            echo "SUCCESS: All files appear to be properly synced!"
          fi
